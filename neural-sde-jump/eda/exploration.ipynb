{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Notebook\n",
    "\n",
    "This will be used primarily so I can understand how GANs work for SDEs and CDEs. This code is ~~stolen from~~ a modified version of [this](https://github.com/google-research/torchsde/blob/master/examples/sde_gan.py) `torchsde` example.\n",
    "\n",
    "I want to translate this to `Diffrax` since `Diffrax` is more robust.\n",
    "\n",
    "### 0: Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim.swa_utils as swa_utils\n",
    "import torchcde\n",
    "import tqdm\n",
    "\n",
    "import torchsde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipSwish(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.909 * torch.nn.functional.silu(x)\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_size, out_size, mlp_size, num_layers, tanh):\n",
    "        super().__init__()\n",
    "\n",
    "        model = [torch.nn.Linear(in_size, mlp_size), LipSwish()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            model.append(torch.nn.Linear(mlp_size, mlp_size))\n",
    "            ###################\n",
    "            # LipSwish activations are useful to constrain the Lipschitz constant of the discriminator.\n",
    "            # (For simplicity we additionally use them in the generator, but that's less important.)\n",
    "            ###################\n",
    "            model.append(LipSwish())\n",
    "        model.append(torch.nn.Linear(mlp_size, out_size))\n",
    "        if tanh:\n",
    "            model.append(torch.nn.Tanh())\n",
    "        self._model = torch.nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Define SDE (Generator)\n",
    "\n",
    "The generator generates paths to be evaluated against the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the general generator function that will be solved for with next function\n",
    "\n",
    "\n",
    "class GeneratorFunc(torch.nn.Module):\n",
    "    sde_type = \"stratonovich\"\n",
    "    noise_type = \"general\"\n",
    "\n",
    "    def __init__(self, noise_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "        self._noise_size = noise_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        ###################\n",
    "        # Drift and diffusion are MLPs. They happen to be the same size.\n",
    "        # Note the final tanh nonlinearity: this is typically important for good performance, to constrain the rate of\n",
    "        # change of the hidden state.\n",
    "        # If you have problems with very high drift/diffusions then consider scaling these so that they squash to e.g.\n",
    "        # [-3, 3] rather than [-1, 1].\n",
    "        ###################\n",
    "        self._drift = MLP(1 + hidden_size, hidden_size, mlp_size, num_layers, tanh=True)\n",
    "        self._diffusion = MLP(\n",
    "            1 + hidden_size, hidden_size * noise_size, mlp_size, num_layers, tanh=True\n",
    "        )\n",
    "\n",
    "    def f_and_g(self, t, x):\n",
    "        # t has shape ()\n",
    "        # x has shape (batch_size, hidden_size)\n",
    "        t = t.expand(x.size(0), 1)\n",
    "        tx = torch.cat([t, x], dim=1)\n",
    "        return self._drift(tx), self._diffusion(tx).view(\n",
    "            x.size(0), self._hidden_size, self._noise_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the SDE from before\n",
    "\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_size,\n",
    "        initial_noise_size,\n",
    "        noise_size,\n",
    "        hidden_size,\n",
    "        mlp_size,\n",
    "        num_layers,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._initial_noise_size = initial_noise_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        self._initial = MLP(\n",
    "            initial_noise_size, hidden_size, mlp_size, num_layers, tanh=False\n",
    "        )\n",
    "        self._func = GeneratorFunc(noise_size, hidden_size, mlp_size, num_layers)\n",
    "        self._readout = torch.nn.Linear(hidden_size, data_size)\n",
    "\n",
    "    def forward(self, ts, batch_size):\n",
    "        # ts has shape (t_size,) and corresponds to the points we want to evaluate the SDE at.\n",
    "\n",
    "        ###################\n",
    "        # Actually solve the SDE.\n",
    "        ###################\n",
    "        init_noise = torch.randn(batch_size, self._initial_noise_size, device=ts.device)\n",
    "        x0 = self._initial(init_noise)\n",
    "\n",
    "        ###################\n",
    "        # We use the reversible Heun method to get accurate gradients whilst using the adjoint method.\n",
    "        ###################\n",
    "        xs = torchsde.sdeint_adjoint(\n",
    "            self._func,\n",
    "            x0,\n",
    "            ts,\n",
    "            method=\"reversible_heun\",\n",
    "            dt=1.0,\n",
    "            adjoint_method=\"adjoint_reversible_heun\",\n",
    "        )\n",
    "        xs = xs.transpose(0, 1)\n",
    "        ys = self._readout(xs)\n",
    "\n",
    "        ###################\n",
    "        # Normalise the data to the form that the discriminator expects, in particular including time as a channel.\n",
    "        ###################\n",
    "        ts = ts.unsqueeze(0).unsqueeze(-1).expand(batch_size, ts.size(0), 1)\n",
    "        return torchcde.linear_interpolation_coeffs(torch.cat([ts, ys], dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Define CDE (Discriminator)\n",
    "\n",
    "The discriminator tells us whether the generated path is close to the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Next the discriminator. Here, we're going to use a neural controlled differential equation (neural CDE) as the\n",
    "# discriminator, just as in the \"Neural SDEs as Infinite-Dimensional GANs\" paper. (You could use other things as well,\n",
    "# but this is a natural choice.)\n",
    "#\n",
    "# There's actually a few different (roughly equivalent) ways of making the discriminator work. The curious reader is\n",
    "# encouraged to have a read of the comment at the bottom of this file for an in-depth explanation.\n",
    "###################\n",
    "class DiscriminatorFunc(torch.nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "        self._data_size = data_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "        # tanh is important for model performance\n",
    "        self._module = MLP(\n",
    "            1 + hidden_size,\n",
    "            hidden_size * (1 + data_size),\n",
    "            mlp_size,\n",
    "            num_layers,\n",
    "            tanh=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, t, h):\n",
    "        # t has shape ()\n",
    "        # h has shape (batch_size, hidden_size)\n",
    "        t = t.expand(h.size(0), 1)\n",
    "        th = torch.cat([t, h], dim=1)\n",
    "        return self._module(th).view(h.size(0), self._hidden_size, 1 + self._data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, mlp_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self._initial = MLP(\n",
    "            1 + data_size, hidden_size, mlp_size, num_layers, tanh=False\n",
    "        )\n",
    "        self._func = DiscriminatorFunc(data_size, hidden_size, mlp_size, num_layers)\n",
    "        self._readout = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, ys_coeffs):\n",
    "        # ys_coeffs has shape (batch_size, t_size, 1 + data_size)\n",
    "        # The +1 corresponds to time. When solving CDEs, It turns out to be most natural to treat time as just another\n",
    "        # channel: in particular this makes handling irregular data quite easy, when the times may be different between\n",
    "        # different samples in the batch.\n",
    "\n",
    "        Y = torchcde.LinearInterpolation(ys_coeffs)\n",
    "        Y0 = Y.evaluate(Y.interval[0])\n",
    "        h0 = self._initial(Y0)\n",
    "        hs = torchcde.cdeint(\n",
    "            Y,\n",
    "            self._func,\n",
    "            h0,\n",
    "            Y.interval,\n",
    "            method=\"reversible_heun\",\n",
    "            backend=\"torchsde\",\n",
    "            dt=1.0,\n",
    "            adjoint_method=\"adjoint_reversible_heun\",\n",
    "            adjoint_params=(ys_coeffs,) + tuple(self._func.parameters()),\n",
    "        )\n",
    "        score = self._readout(hs[:, -1])\n",
    "        return score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Data Generator\n",
    "\n",
    "This is unrelated to the previous generator. This \"Generator\" creates the data. In practice one would simply import the data from an external source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Generate some data. For this example we generate some synthetic data from a time-dependent Ornstein-Uhlenbeck SDE.\n",
    "###################\n",
    "def get_data(batch_size, device):\n",
    "    dataset_size = 8192\n",
    "    t_size = 64\n",
    "\n",
    "    class OrnsteinUhlenbeckSDE(torch.nn.Module):\n",
    "        sde_type = \"ito\"\n",
    "        noise_type = \"scalar\"\n",
    "\n",
    "        def __init__(self, mu, theta, sigma):\n",
    "            super().__init__()\n",
    "            self.register_buffer(\"mu\", torch.as_tensor(mu))\n",
    "            self.register_buffer(\"theta\", torch.as_tensor(theta))\n",
    "            self.register_buffer(\"sigma\", torch.as_tensor(sigma))\n",
    "\n",
    "        def f(self, t, y):\n",
    "            return self.mu * t - self.theta * y\n",
    "\n",
    "        def g(self, t, y):\n",
    "            return self.sigma.expand(y.size(0), 1, 1) * (2 * t / t_size)\n",
    "\n",
    "    ou_sde = OrnsteinUhlenbeckSDE(mu=0.02, theta=0.1, sigma=0.4).to(device)\n",
    "    y0 = torch.rand(dataset_size, device=device).unsqueeze(-1) * 2 - 1\n",
    "    ts = torch.linspace(0, t_size - 1, t_size, device=device)\n",
    "    ys = torchsde.sdeint(ou_sde, y0, ts, dt=1e-1)\n",
    "\n",
    "    ###################\n",
    "    # To demonstrate how to handle irregular data, then here we additionally drop some of the data (by setting it to\n",
    "    # NaN.)\n",
    "    ###################\n",
    "    ys_num = ys.numel()\n",
    "    to_drop = torch.randperm(ys_num)[: int(0.3 * ys_num)]\n",
    "    ys.view(-1)[to_drop] = float(\"nan\")\n",
    "\n",
    "    ###################\n",
    "    # Typically important to normalise data. Note that the data is normalised with respect to the statistics of the\n",
    "    # initial data, _not_ the whole time series. This seems to help the learning process, presumably because if the\n",
    "    # initial condition is wrong then it's pretty hard to learn the rest of the SDE correctly.\n",
    "    ###################\n",
    "    y0_flat = ys[0].view(-1)\n",
    "    y0_not_nan = y0_flat.masked_select(~torch.isnan(y0_flat))\n",
    "    ys = (ys - y0_not_nan.mean()) / y0_not_nan.std()\n",
    "\n",
    "    ###################\n",
    "    # As discussed, time must be included as a channel for the discriminator.\n",
    "    ###################\n",
    "    ys = torch.cat(\n",
    "        [\n",
    "            ts.unsqueeze(0).unsqueeze(-1).expand(dataset_size, t_size, 1),\n",
    "            ys.transpose(0, 1),\n",
    "        ],\n",
    "        dim=2,\n",
    "    )\n",
    "    # shape (dataset_size=1000, t_size=100, 1 + data_size=3)\n",
    "\n",
    "    ###################\n",
    "    # Package up.\n",
    "    ###################\n",
    "    data_size = (\n",
    "        ys.size(-1) - 1\n",
    "    )  # How many channels the data has (not including time, hence the minus one).\n",
    "    ys_coeffs = torchcde.linear_interpolation_coeffs(ys)  # as per neural CDEs.\n",
    "    dataset = torch.utils.data.TensorDataset(ys_coeffs)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    return ts, data_size, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# We'll plot some results at the end.\n",
    "###################\n",
    "def plot(ts, generator, dataloader, num_plot_samples, plot_locs):\n",
    "    # Get samples\n",
    "    (real_samples,) = next(iter(dataloader))\n",
    "    assert num_plot_samples <= real_samples.size(0)\n",
    "    real_samples = torchcde.LinearInterpolation(real_samples).evaluate(ts)\n",
    "    real_samples = real_samples[..., 1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_samples = generator(ts, real_samples.size(0)).cpu()\n",
    "    generated_samples = torchcde.LinearInterpolation(generated_samples).evaluate(ts)\n",
    "    generated_samples = generated_samples[..., 1]\n",
    "\n",
    "    # Plot histograms\n",
    "    for prop in plot_locs:\n",
    "        time = int(prop * (real_samples.size(1) - 1))\n",
    "        real_samples_time = real_samples[:, time]\n",
    "        generated_samples_time = generated_samples[:, time]\n",
    "        _, bins, _ = plt.hist(\n",
    "            real_samples_time.cpu().numpy(),\n",
    "            bins=32,\n",
    "            alpha=0.7,\n",
    "            label=\"Real\",\n",
    "            color=\"dodgerblue\",\n",
    "            density=True,\n",
    "        )\n",
    "        bin_width = bins[1] - bins[0]\n",
    "        num_bins = int(\n",
    "            (generated_samples_time.max() - generated_samples_time.min()).item()\n",
    "            // bin_width\n",
    "        )\n",
    "        plt.hist(\n",
    "            generated_samples_time.cpu().numpy(),\n",
    "            bins=num_bins,\n",
    "            alpha=0.7,\n",
    "            label=\"Generated\",\n",
    "            color=\"crimson\",\n",
    "            density=True,\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.title(f\"Marginal distribution at time {time}.\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    real_samples = real_samples[:num_plot_samples]\n",
    "    generated_samples = generated_samples[:num_plot_samples]\n",
    "\n",
    "    # Plot samples\n",
    "    real_first = True\n",
    "    generated_first = True\n",
    "    for real_sample_ in real_samples:\n",
    "        kwargs = {\"label\": \"Real\"} if real_first else {}\n",
    "        plt.plot(\n",
    "            ts.cpu(),\n",
    "            real_sample_.cpu(),\n",
    "            color=\"dodgerblue\",\n",
    "            linewidth=0.5,\n",
    "            alpha=0.7,\n",
    "            **kwargs,\n",
    "        )\n",
    "        real_first = False\n",
    "    for generated_sample_ in generated_samples:\n",
    "        kwargs = {\"label\": \"Generated\"} if generated_first else {}\n",
    "        plt.plot(\n",
    "            ts.cpu(),\n",
    "            generated_sample_.cpu(),\n",
    "            color=\"crimson\",\n",
    "            linewidth=0.5,\n",
    "            alpha=0.7,\n",
    "            **kwargs,\n",
    "        )\n",
    "        generated_first = False\n",
    "    plt.legend()\n",
    "    plt.title(f\"{num_plot_samples} samples from both real and generated distributions.\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(ts, batch_size, dataloader, generator, discriminator):\n",
    "    with torch.no_grad():\n",
    "        total_samples = 0\n",
    "        total_loss = 0\n",
    "        for (real_samples,) in dataloader:\n",
    "            generated_samples = generator(ts, batch_size)\n",
    "            generated_score = discriminator(generated_samples)\n",
    "            real_score = discriminator(real_samples)\n",
    "            loss = generated_score - real_score\n",
    "            total_samples += batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 149\u001b[0m\n\u001b[1;32m    145\u001b[0m     plot(ts, generator, test_dataloader, num_plot_samples, plot_locs)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[43mfire\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Neural SDEs with Jump/.venv/lib/python3.11/site-packages/fire/core.py:135\u001b[0m, in \u001b[0;36mFire\u001b[0;34m(component, command, name, serialize)\u001b[0m\n\u001b[1;32m    132\u001b[0m   context\u001b[38;5;241m.\u001b[39mupdate(caller_globals)\n\u001b[1;32m    133\u001b[0m   context\u001b[38;5;241m.\u001b[39mupdate(caller_locals)\n\u001b[0;32m--> 135\u001b[0m component_trace \u001b[38;5;241m=\u001b[39m \u001b[43m_Fire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparsed_flag_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component_trace\u001b[38;5;241m.\u001b[39mHasError():\n\u001b[1;32m    138\u001b[0m   _DisplayError(component_trace)\n",
      "File \u001b[0;32m~/Desktop/Neural SDEs with Jump/.venv/lib/python3.11/site-packages/fire/core.py:468\u001b[0m, in \u001b[0;36m_Fire\u001b[0;34m(component, args, parsed_flag_args, context, name)\u001b[0m\n\u001b[1;32m    465\u001b[0m is_class \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39misclass(component)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m   component, remaining_args \u001b[38;5;241m=\u001b[39m \u001b[43m_CallAndUpdateTrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m      \u001b[49m\u001b[43mremaining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomponent_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroutine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m   handled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FireError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/Desktop/Neural SDEs with Jump/.venv/lib/python3.11/site-packages/fire/core.py:684\u001b[0m, in \u001b[0;36m_CallAndUpdateTrace\u001b[0;34m(component, args, component_trace, treatment, target)\u001b[0m\n\u001b[1;32m    682\u001b[0m   component \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(fn(\u001b[38;5;241m*\u001b[39mvarargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 684\u001b[0m   component \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvarargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m treatment \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    687\u001b[0m   action \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mINSTANTIATED_CLASS\n",
      "Cell \u001b[0;32mIn[20], line 51\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(initial_noise_size, noise_size, hidden_size, mlp_size, num_layers, generator_lr, discriminator_lr, batch_size, steps, init_mult1, init_mult2, weight_decay, swa_step_start, steps_per_print, num_plot_samples, plot_locs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: MPS not available; falling back to CPU but this is likely to be very slow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Data\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m ts, data_size, train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m infinite_train_dataloader \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     53\u001b[0m     elem \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m: train_dataloader, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Models\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(batch_size, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m y0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(dataset_size, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m ts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, t_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, t_size, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 27\u001b[0m ys \u001b[38;5;241m=\u001b[39m \u001b[43mtorchsde\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mou_sde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# To demonstrate how to handle irregular data, then here we additionally drop some of the data (by setting it to\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# NaN.)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[1;32m     33\u001b[0m ys_num \u001b[38;5;241m=\u001b[39m ys\u001b[38;5;241m.\u001b[39mnumel()\n",
      "File \u001b[0;32m~/Desktop/Neural SDEs with Jump/.venv/lib/python3.11/site-packages/torchsde/_core/sdeint.py:110\u001b[0m, in \u001b[0;36msdeint\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_solver_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     extra_solver_state \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39minit_extra_solver_state(ts[\u001b[38;5;241m0\u001b[39m], y0)\n\u001b[0;32m--> 110\u001b[0m ys, extra_solver_state \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_solver_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parse_return(y0, ys, extra_solver_state, extra, logqp)\n",
      "File \u001b[0;32m~/Desktop/Neural SDEs with Jump/.venv/lib/python3.11/site-packages/torchsde/_core/base_solver.py:145\u001b[0m, in \u001b[0;36mBaseSDESolver.integrate\u001b[0;34m(self, y0, ts, extra0)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         prev_t, prev_y \u001b[38;5;241m=\u001b[39m curr_t, curr_y\n\u001b[0;32m--> 145\u001b[0m         curr_y, curr_extra \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_extra\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m         curr_t \u001b[38;5;241m=\u001b[39m next_t\n\u001b[1;32m    147\u001b[0m ys\u001b[38;5;241m.\u001b[39mappend(interp\u001b[38;5;241m.\u001b[39mlinear_interp(t0\u001b[38;5;241m=\u001b[39mprev_t, y0\u001b[38;5;241m=\u001b[39mprev_y, t1\u001b[38;5;241m=\u001b[39mcurr_t, y1\u001b[38;5;241m=\u001b[39mcurr_y, t\u001b[38;5;241m=\u001b[39mout_t))\n",
      "File \u001b[0;32m~/Desktop/Neural SDEs with Jump/.venv/lib/python3.11/site-packages/torchsde/_core/methods/srk.py:75\u001b[0m, in \u001b[0;36mSRK.diagonal_or_scalar_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     73\u001b[0m     g \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m g\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m g\n\u001b[1;32m     74\u001b[0m     H0s \u001b[38;5;241m=\u001b[39m H0s \u001b[38;5;241m+\u001b[39m srid2\u001b[38;5;241m.\u001b[39mA0[s][j] \u001b[38;5;241m*\u001b[39m f \u001b[38;5;241m*\u001b[39m dt \u001b[38;5;241m+\u001b[39m srid2\u001b[38;5;241m.\u001b[39mB0[s][j] \u001b[38;5;241m*\u001b[39m g \u001b[38;5;241m*\u001b[39m I_k0 \u001b[38;5;241m*\u001b[39m rdt\n\u001b[0;32m---> 75\u001b[0m     H1s \u001b[38;5;241m=\u001b[39m H1s \u001b[38;5;241m+\u001b[39m srid2\u001b[38;5;241m.\u001b[39mA1[s][j] \u001b[38;5;241m*\u001b[39m f \u001b[38;5;241m*\u001b[39m dt \u001b[38;5;241m+\u001b[39m srid2\u001b[38;5;241m.\u001b[39mB1[s][j] \u001b[38;5;241m*\u001b[39m g \u001b[38;5;241m*\u001b[39m sqrt_dt\n\u001b[1;32m     76\u001b[0m H0\u001b[38;5;241m.\u001b[39mappend(H0s)\n\u001b[1;32m     77\u001b[0m H1\u001b[38;5;241m.\u001b[39mappend(H1s)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Now do normal GAN training, and plot the results.\n",
    "#\n",
    "# GANs are famously tricky and SDEs trained as GANs are no exception. Hopefully you can learn from our experience and\n",
    "# get these working faster than we did -- we found that several tricks were often helpful to get this working in a\n",
    "# reasonable fashion:\n",
    "# - Stochastic weight averaging (average out the oscillations in GAN training).\n",
    "# - Weight decay (reduce the oscillations in GAN training).\n",
    "# - Final tanh nonlinearities in the architectures of the vector fields, as above. (To avoid the model blowing up.)\n",
    "# - Adadelta (interestingly seems to be a lot better than either SGD or Adam).\n",
    "# - Choosing a good learning rate (always important).\n",
    "# - Scaling the weights at initialisation to be roughly the right size (chosen through empirical trial-and-error).\n",
    "###################\n",
    "\n",
    "\n",
    "def main(\n",
    "    # Architectural hyperparameters. These are quite small for illustrative purposes.\n",
    "    initial_noise_size=5,  # How many noise dimensions to sample at the start of the SDE.\n",
    "    noise_size=3,  # How many dimensions the Brownian motion has.\n",
    "    hidden_size=16,  # How big the hidden size of the generator SDE and the discriminator CDE are.\n",
    "    mlp_size=16,  # How big the layers in the various MLPs are.\n",
    "    num_layers=1,  # How many hidden layers to have in the various MLPs.\n",
    "    # Training hyperparameters. Be prepared to tune these very carefully, as with any GAN.\n",
    "    generator_lr=2e-4,  # Learning rate often needs careful tuning to the problem.\n",
    "    discriminator_lr=1e-3,  # Learning rate often needs careful tuning to the problem.\n",
    "    batch_size=1024,  # Batch size.\n",
    "    steps=10000,  # How many steps to train both generator and discriminator for.\n",
    "    init_mult1=3,  # Changing the initial parameter size can help.\n",
    "    init_mult2=0.5,  #\n",
    "    weight_decay=0.01,  # Weight decay.\n",
    "    swa_step_start=5000,  # When to start using stochastic weight averaging.\n",
    "    # Evaluation and plotting hyperparameters\n",
    "    steps_per_print=10,  # How often to print the loss.\n",
    "    num_plot_samples=50,  # How many samples to use on the plots at the end.\n",
    "    plot_locs=(\n",
    "        0.1,\n",
    "        0.3,\n",
    "        0.5,\n",
    "        0.7,\n",
    "        0.9,\n",
    "    ),  # Plot some marginal distributions at this proportion of the way along.\n",
    "):\n",
    "    is_mps = torch.backends.mps.is_available()\n",
    "    device = \"mps\" if is_mps else \"cpu\"\n",
    "    if not is_mps:\n",
    "        print(\n",
    "            \"Warning: MPS not available; falling back to CPU but this is likely to be very slow.\"\n",
    "        )\n",
    "\n",
    "    # Data\n",
    "    ts, data_size, train_dataloader = get_data(batch_size=batch_size, device=device)\n",
    "    infinite_train_dataloader = (\n",
    "        elem for it in iter(lambda: train_dataloader, None) for elem in it\n",
    "    )\n",
    "\n",
    "    # Models\n",
    "    generator = Generator(\n",
    "        data_size, initial_noise_size, noise_size, hidden_size, mlp_size, num_layers\n",
    "    ).to(device)\n",
    "    discriminator = Discriminator(data_size, hidden_size, mlp_size, num_layers).to(\n",
    "        device\n",
    "    )\n",
    "    # Weight averaging really helps with GAN training.\n",
    "    averaged_generator = swa_utils.AveragedModel(generator)\n",
    "    averaged_discriminator = swa_utils.AveragedModel(discriminator)\n",
    "\n",
    "    # Picking a good initialisation is important!\n",
    "    # In this case these were picked by making the parameters for the t=0 part of the generator be roughly the right\n",
    "    # size that the untrained t=0 distribution has a similar variance to the t=0 data distribution.\n",
    "    # Then the func parameters were adjusted so that the t>0 distribution looked like it had about the right variance.\n",
    "    # What we're doing here is very crude -- one can definitely imagine smarter ways of doing things.\n",
    "    # (e.g. pretraining the t=0 distribution)\n",
    "    with torch.no_grad():\n",
    "        for param in generator._initial.parameters():\n",
    "            param *= init_mult1\n",
    "        for param in generator._func.parameters():\n",
    "            param *= init_mult2\n",
    "\n",
    "    # Optimisers. Adadelta turns out to be a much better choice than SGD or Adam, interestingly.\n",
    "    generator_optimiser = torch.optim.Adadelta(\n",
    "        generator.parameters(), lr=generator_lr, weight_decay=weight_decay\n",
    "    )\n",
    "    discriminator_optimiser = torch.optim.Adadelta(\n",
    "        discriminator.parameters(), lr=discriminator_lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Train both generator and discriminator.\n",
    "    trange = tqdm.tqdm(range(steps))\n",
    "    for step in trange:\n",
    "        (real_samples,) = next(infinite_train_dataloader)\n",
    "\n",
    "        generated_samples = generator(ts, batch_size)\n",
    "        generated_score = discriminator(generated_samples)\n",
    "        real_score = discriminator(real_samples)\n",
    "        loss = generated_score - real_score\n",
    "        loss.backward()\n",
    "\n",
    "        for param in generator.parameters():\n",
    "            param.grad *= -1\n",
    "        generator_optimiser.step()\n",
    "        discriminator_optimiser.step()\n",
    "        generator_optimiser.zero_grad()\n",
    "        discriminator_optimiser.zero_grad()\n",
    "\n",
    "        ###################\n",
    "        # We constrain the Lipschitz constant of the discriminator using carefully-chosen clipping (and the use of\n",
    "        # LipSwish activation functions).\n",
    "        ###################\n",
    "        with torch.no_grad():\n",
    "            for module in discriminator.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    lim = 1 / module.out_features\n",
    "                    module.weight.clamp_(-lim, lim)\n",
    "\n",
    "        # Stochastic weight averaging typically improves performance.\n",
    "        if step > swa_step_start:\n",
    "            averaged_generator.update_parameters(generator)\n",
    "            averaged_discriminator.update_parameters(discriminator)\n",
    "\n",
    "        if (step % steps_per_print) == 0 or step == steps - 1:\n",
    "            total_unaveraged_loss = evaluate_loss(\n",
    "                ts, batch_size, train_dataloader, generator, discriminator\n",
    "            )\n",
    "            if step > swa_step_start:\n",
    "                total_averaged_loss = evaluate_loss(\n",
    "                    ts,\n",
    "                    batch_size,\n",
    "                    train_dataloader,\n",
    "                    averaged_generator.module,\n",
    "                    averaged_discriminator.module,\n",
    "                )\n",
    "                trange.write(\n",
    "                    f\"Step: {step:3} Loss (unaveraged): {total_unaveraged_loss:.4f} \"\n",
    "                    f\"Loss (averaged): {total_averaged_loss:.4f}\"\n",
    "                )\n",
    "            else:\n",
    "                trange.write(\n",
    "                    f\"Step: {step:3} Loss (unaveraged): {total_unaveraged_loss:.4f}\"\n",
    "                )\n",
    "    generator.load_state_dict(averaged_generator.module.state_dict())\n",
    "    discriminator.load_state_dict(averaged_discriminator.module.state_dict())\n",
    "\n",
    "    _, _, test_dataloader = get_data(batch_size=batch_size, device=device)\n",
    "\n",
    "    plot(ts, generator, test_dataloader, num_plot_samples, plot_locs)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
